{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "211725b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.io import loadmat\n",
    "from sklearn import svm\n",
    "import re\n",
    "from stemming.porter2 import stem\n",
    "import nltk, nltk.stem.porter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f03cd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3310ccfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Anyone knows how much it costs to host a web portal ?\r\n",
      ">\r\n",
      "Well, it depends on how many visitors you're expecting.\r\n",
      "This can be anywhere from less than 10 bucks a month to a couple of $100. \r\n",
      "You should checkout http://www.rackspace.com/ or perhaps Amazon EC2 \r\n",
      "if youre running something big..\r\n",
      "\r\n",
      "To unsubscribe yourself from this mailing list, send an email to:\r\n",
      "groupname-unsubscribe@egroups.com\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./data/emailSample1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f01a7d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(email):\n",
    "    \"\"\"\n",
    "    Function to do some pre processing (simplification of e-mails).\n",
    "    Comments throughout implementation describe what it does.\n",
    "    Input = raw e-mail\n",
    "    Output = processed (simplified) email\n",
    "    \"\"\"\n",
    "    # Make the entire e-mail lower case\n",
    "    email = email.lower()\n",
    "    \n",
    "    # Strip html tags (strings that look like <blah> where 'blah' does not\n",
    "    # contain '<' or '>')... replace with a space\n",
    "    email = re.sub('<[^<>]+>', ' ', email);\n",
    "    \n",
    "    #Any numbers get replaced with the string 'number'\n",
    "    email = re.sub('[0-9]+', 'number', email)\n",
    "    \n",
    "    #Anything starting with http or https:// replaced with 'httpaddr'\n",
    "    email = re.sub('(http|https)://[^\\s]*', 'httpaddr', email)\n",
    "    \n",
    "    #Strings with \"@\" in the middle are considered emails --> 'emailaddr'\n",
    "    email = re.sub('[^\\s]+@[^\\s]+', 'emailaddr', email);\n",
    "    \n",
    "    #The '$' sign gets replaced with 'dollar'\n",
    "    email = re.sub('[$]+', 'dollar', email);\n",
    "    \n",
    "    return email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f40d8b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"> anyone knows how much it costs to host a web portal ?\\n>\\nwell, it depends on how many visitors you're expecting.\\nthis can be anywhere from less than number bucks a month to a couple of dollarnumber. \\nyou should checkout httpaddr or perhaps amazon ecnumber \\nif youre running something big..\\n\\nto unsubscribe yourself from this mailing list, send an email to:\\nemailaddr\\n\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_contents = open('./data/emailSample1.txt').read()\n",
    "preProcess(email_contents)                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66345c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def email2TokenList( raw_email ):\n",
    "    \"\"\"\n",
    "    Function that takes in preprocessed (simplified) email, tokenizes it,\n",
    "    stems each word, and returns an (ordered) list of tokens in the e-mail\n",
    "    提取出预处理后的email中的每个单词，并加入到list中\n",
    "    \"\"\"\n",
    "    \n",
    "    # I'll use the NLTK stemmer because it more accurately duplicates the\n",
    "    # performance of the OCTAVE implementation in the assignment\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    \n",
    "    email = preProcess( raw_email )\n",
    "\n",
    "    #Split the e-mail into individual words (tokens) (split by the delimiter ' ')\n",
    "    #but also split by delimiters '@', '$', '/', etc etc\n",
    "    #Splitting by many delimiters is easiest with re.split()\n",
    "    tokens = re.split('[ \\@\\$\\/\\#\\.\\-\\:\\&\\*\\+\\=\\[\\]\\?\\!\\(\\)\\{\\}\\,\\'\\\"\\>\\_\\<\\;\\%]', email)\n",
    "    \n",
    "    #Loop over each word (token) and use a stemmer to shorten it,\n",
    "    #then check if the word is in the vocab_list... if it is,\n",
    "    #store what index in the vocab_list the word is\n",
    "    tokenlist = []\n",
    "    for token in tokens:\n",
    "        \n",
    "        #Remove any non alphanumeric characters\n",
    "        token = re.sub('[^a-zA-Z0-9]', '', token);\n",
    "\n",
    "        #Use the Porter stemmer to stem the word\n",
    "        stemmed = stemmer.stem( token )\n",
    "        \n",
    "        #Throw out empty tokens\n",
    "        if not len(token): continue\n",
    "            \n",
    "        #Store a list of all unique stemmed words\n",
    "        tokenlist.append(stemmed)\n",
    "            \n",
    "    return tokenlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c54bacc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVocabDict(reverse=False):\n",
    "    \"\"\"\n",
    "    Function to read in the supplied vocab list text file into a dictionary.\n",
    "    I'll use this for now, but since I'm using a slightly different stemmer,\n",
    "    I'd like to generate this list myself from some sort of data set...\n",
    "    Dictionary key is the stemmed word, value is the index in the text file\n",
    "    If \"reverse\", the keys and values are switched.\n",
    "    读取vocab list并将其转换成dictionary\n",
    "    \"\"\"\n",
    "    vocab_dict = {}\n",
    "    with open(\"./data/vocab.txt\") as f:\n",
    "        for line in f:\n",
    "            (val, key) = line.split()\n",
    "            if not reverse:\n",
    "                vocab_dict[key] = int(val)\n",
    "            else:\n",
    "                vocab_dict[int(val)] = key\n",
    "                \n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b14f9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def email2VocabIndices( raw_email, vocab_dict ):\n",
    "    \"\"\"\n",
    "    Function that takes in a raw email and returns a list of indices corresponding\n",
    "    to the location in vocab_dict for each stemmed word in the email.\n",
    "    如果email中的单词出现在vocab_dict中，则将该单词对应的索引加入list，否则跳过\n",
    "    \"\"\"\n",
    "    tokenlist = email2TokenList( raw_email )\n",
    "    index_list = [ vocab_dict[token] for token in tokenlist if token in vocab_dict ]\n",
    "    return index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5544167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Features from Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a9fb5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def email2FeatureVector( raw_email, vocab_dict ):\n",
    "    \"\"\"\n",
    "    Function that takes as input a raw email, and returns a vector of shape\n",
    "    (n,1) where n is the size of the vocab_dict.\n",
    "    The first element in this vector is 1 if the vocab word with index == 1\n",
    "    is in the raw_email, 0 otherwise.\n",
    "    将特征向量化，向量的长度为vocab_dict的大小。\n",
    "    如果email中的单词出现在vocab_dict中，则将向量中对应位置的分量置为1，否则置为0\n",
    "    \"\"\"\n",
    "    n = len(vocab_dict)\n",
    "    result = np.zeros((n,1))\n",
    "    vocab_indices = email2VocabIndices( email_contents, vocab_dict ) # 返回index\n",
    "    for idx in vocab_indices:\n",
    "        result[idx] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abbe3718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1899, 45.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict = getVocabDict()\n",
    "email_contents = open('./data/emailSample1.txt').read()\n",
    "feature_v = email2FeatureVector(email_contents, vocab_dict)\n",
    "len(feature_v), np.sum(feature_v)                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1b3ea50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4000, 1899), (4000, 1), (1000, 1899), (1000, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './data/spamTrain.mat'\n",
    "path2 = './data/spamTest.mat'\n",
    "\n",
    "train_data = loadmat(path)\n",
    "X = train_data['X']\n",
    "y = train_data['y']\n",
    "\n",
    "test_data = loadmat(path2)\n",
    "Xtest = test_data['Xtest']\n",
    "ytest = test_data['ytest']\n",
    "\n",
    "X.shape, y.shape, Xtest.shape, ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d283e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1277, 1899), (2723, 1899))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pos = np.array([X[i] for i in range(X.shape[0]) if y[i] == 1])\n",
    "X_neg = np.array([X[i] for i in range(X.shape[0]) if y[i] == 0])\n",
    "X_pos.shape, X_neg.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33fa517d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.1, kernel='linear')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = 0.1\n",
    "linear_svm = svm.SVC(C=C, kernel='linear')\n",
    "linear_svm.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b04272c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acuuracy: 99.825%\n",
      "Test accuracy: 98.9%\n"
     ]
    }
   ],
   "source": [
    "# 训练集准确率\n",
    "train_pred = linear_svm.score(X, y)\n",
    "\n",
    "# 测试集准确率\n",
    "test_pred = linear_svm.score(Xtest, ytest)\n",
    "\n",
    "print(f'Train acuuracy: {train_pred * 100}%')\n",
    "print(f'Test accuracy: {test_pred * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98d0964b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=uint8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对前面处理过的邮件的预测\n",
    "linear_svm.predict(feature_v.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a5757af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 15 most important words to classify a spam e-mail are:\n",
      "['otherwis', 'clearli', 'remot', 'gt', 'visa', 'base', 'doesn', 'wife', 'previous', 'player', 'mortgag', 'natur', 'll', 'futur', 'hot']\n"
     ]
    }
   ],
   "source": [
    "# Determine the words most likely to indicate an e-mail is a spam\n",
    "# From the trained SVM we can get a list of the weight coefficients for each\n",
    "# word (technically, each word index)\n",
    "\n",
    "vocab_dict_flipped = getVocabDict(reverse=True)\n",
    "\n",
    "# Sort indicies from most important to least-important (high to low weight)\n",
    "# The absolute size of the coefficient relative to the other ones gives an indication of how important the feature was for the separation. \n",
    "sorted_indices = np.argsort( linear_svm.coef_, axis=None )[::-1]\n",
    "print (\"The 15 most important words to classify a spam e-mail are:\")\n",
    "print ([ vocab_dict_flipped[x] for x in sorted_indices[:15] ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
